{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Navya003/NLP/blob/main/XGBoost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "## Install the latest version of wandb client ðŸ”¥ðŸ”¥\n",
        "!pip install -q --upgrade wandb xgboost\n"
      ],
      "metadata": {
        "id": "UVuO7Gs14fP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import wandb\n",
        "from wandb.integration.xgboost import WandbCallback\n",
        "\n",
        "# Paste your api key here\n",
        "os.environ[\"WANDB_API_KEY\"] = '063315dd7a544719f472794859a0805bc5f26fa2'\n",
        "\n",
        "wandb.init(project='fc-xgboost')\n",
        "\n",
        "# Feel free to change these and experiment !!\n",
        "config = wandb.config\n",
        "config.seed = 123\n",
        "config.test_size = 0.2\n",
        "config.colsample_bytree = 0.3\n",
        "config.learning_rate = 0.01\n",
        "config.max_depth = 15\n",
        "config.alpha = 10\n",
        "config.n_estimators = 5\n",
        "\n",
        "wandb.config.update(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "Lzh-OW164yoI",
        "outputId": "4e7fdd0a-9a84-45cb-f798-16f29411b5b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-a40221cfcefe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintegration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxgboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWandbCallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Paste your api key here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wandb/integration/xgboost/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \"\"\"\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mxgboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwandb_callback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWandbCallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0m__all__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"wandb_callback\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"WandbCallback\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wandb/integration/xgboost/xgboost.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mWandbCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainingCallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \"\"\"`WandbCallback` automatically integrates XGBoost with wandb.\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'xgboost.callback' has no attribute 'TrainingCallback'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5szBwzplw-s1",
        "outputId": "5e4b67f7-9a0a-4184-f530-97cefa340c14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The training dataset has 65 records.\n",
            "The testing dataset has 33 records.\n",
            "The recall value for the baseline xgboost model is 0.6250\n",
            "The best score is 0.8485\n",
            "The best score standard deviation is 0.0429\n",
            "The best hyperparameters are {'colsample_bytree': 0.3, 'reg_alpha': 0, 'reg_lambda': 0}\n",
            "The recall value for the xgboost grid search is 0.6875\n",
            "The best score is 0.8182\n",
            "The best score standard deviation is 0.0742\n",
            "The best hyperparameters are {'reg_lambda': 1e-05, 'reg_alpha': 1e-05, 'max_depth': 3, 'learning_rate': 0.1, 'gamma': 0.0, 'colsample_bytree': 0.5}\n",
            "The recall value for the xgboost random search is 0.7500\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:04<00:00,  9.72it/s, best loss: -0.9090909090909091]\n",
            "{'colsample_bytree': 0, 'gamma': 1, 'learning_rate': 2, 'max_depth': 4, 'reg_alpha': 2, 'reg_lambda': 5}\n",
            "{'colsample_bytree': 0.3, 'gamma': 0.1, 'learning_rate': 0.01, 'max_depth': 15, 'reg_alpha': 0.1, 'reg_lambda': 100}\n",
            "The recall value for the xgboost Bayesian optimization is 0.3750\n"
          ]
        }
      ],
      "source": [
        "###### Step 1: Install And Import Libraries\n",
        "# Data processing\n",
        "import numpy\n",
        "# Model and performance evaluation\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import precision_recall_fscore_support as score\n",
        "# Hyperparameter tuning\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
        "from hyperopt import tpe, STATUS_OK, Trials, hp, fmin, STATUS_OK, space_eval # pip3 install hyperopt\n",
        "\n",
        "\n",
        "###### Step 2: Read In Data\n",
        "# load data\n",
        "f_pos = open('/content/Positive.w2v','r') # opened the word embeddings file trained on natural sequences(positive data) from dna2vec\n",
        "f_neg = open('/content/Negative.w2v','r') # opened the word embeddings file trained on synthetic sequences(negative data)from dna2vec\n",
        "fcontent_pos = f_pos.read() # read content on positive data\n",
        "fcontent_neg = f_neg.read() # read content on negative data\n",
        "lis_pos = [x.split() for x in fcontent_pos.split('\\n')[1:50]] # took content from positive data in form of list of sequence embeddings separated by line from second line to last line # excluded first line here because it is not desired output from dna2vec-it is just the matrix dimension of resulting embeddings\n",
        "lis1_pos = [[float(x) for x in y[1:]] for y in lis_pos] # converted the list elements to float(numerical values) from strting(default datatype when read from file)- here we had left out k-mer such as AAA since that is of no need- we only need embeddings(vector)-that is why we had included from elements first value i.e y[1:]\n",
        "lis_neg  = [x.split() for x in fcontent_neg.split('\\n')[1:50]] # # took content from negative data\n",
        "lis1_neg = [[float(x) for x in y[1:]] for y in lis_neg] # converted the list elements to float(numerical values) from strting(default datatype when read from file)- here we had left out k-mer such as AAA since that is of no need- we only need embeddings(vector)-that is why we had included from elements first value i.e y[1:]\n",
        "l_pos = [x+[1] for x in lis1_pos] # labelled natural sequence embeddings as 1\n",
        "l_neg = [x+[0] for x in lis1_neg] # labelled synthetic sequence embeddings as 0\n",
        "l_whole = l_pos+l_neg # merged both list containing positive sequence embeddings and negative\n",
        "dataset = numpy.array([numpy.array(x) for x in l_whole]) # converted the dataset into arrays for XGBoost implememtation\n",
        "\n",
        "# split data into X and Y\n",
        "X = dataset[:,0:-1] # X is sequence embeddings which needs to be classified\n",
        "Y = dataset[:,-1] # Y is label of sequence embeddings\n",
        "# split data into train and test sets\n",
        "seed = 7 # random state is defined for making training less bias prone\n",
        "test_size = 0.33 # test dataset is 1/3 of dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=seed) # splitted dataset into training and testing data\n",
        "\n",
        "# Check the number of records in training and testing dataset.\n",
        "print(f'The training dataset has {len(X_train)} records.')\n",
        "print(f'The testing dataset has {len(X_test)} records.')\n",
        "\n",
        "###### Step 3: XGBoost Classifier With No Hyperparameter Tuning\n",
        "# Initiate XGBoost Classifier\n",
        "xgboost = XGBClassifier()\n",
        "# Print default setting\n",
        "xgboost.get_params()\n",
        "# Train the model\n",
        "xgboost = XGBClassifier(seed=0).fit(X_train,y_train)\n",
        "# Make prediction\n",
        "xgboost_predict = xgboost.predict(X_test)\n",
        "# Get predicted probability\n",
        "xgboost_predict_prob = xgboost.predict_proba(X_test)[:,1]\n",
        "# Get performance metrics\n",
        "precision, recall, fscore, support = score(y_test, xgboost_predict)\n",
        "# Print result\n",
        "print(f'The recall value for the baseline xgboost model is {recall[1]:.4f}')\n",
        "\n",
        "###### Step 4: Grid Search for XGBoost\n",
        "# Define the search space\n",
        "param_grid = {\n",
        "    # Percentage of columns to be randomly samples for each tree.\n",
        "    \"colsample_bytree\": [ 0.3, 0.5 , 0.8 ],\n",
        "    # reg_alpha provides l1 regularization to the weight, higher values result in more conservative models\n",
        "    \"reg_alpha\": [0, 0.5, 1, 5],\n",
        "    # reg_lambda provides l2 regularization to the weight, higher values result in more conservative models\n",
        "    \"reg_lambda\": [0, 0.5, 1, 5]\n",
        "    }\n",
        "\n",
        "# Set up score\n",
        "scoring = ['recall']\n",
        "# Set up the k-fold cross-validation\n",
        "kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=0)\n",
        "# Define grid search\n",
        "grid_search = GridSearchCV(estimator=xgboost,\n",
        "                           param_grid=param_grid,\n",
        "                           scoring=scoring,\n",
        "                           refit='recall',\n",
        "                           n_jobs=-1,\n",
        "                           cv=kfold,\n",
        "                           verbose=0)\n",
        "# Fit grid search\n",
        "grid_result = grid_search.fit(X_train, y_train)\n",
        "# Print grid search summary\n",
        "grid_result\n",
        "# Print the best score and the corresponding hyperparameters\n",
        "print(f'The best score is {grid_result.best_score_:.4f}')\n",
        "print('The best score standard deviation is', round(grid_result.cv_results_['std_test_recall'][grid_result.best_index_], 4))\n",
        "print(f'The best hyperparameters are {grid_result.best_params_}')\n",
        "# Make prediction using the best model\n",
        "grid_predict = grid_search.predict(X_test)\n",
        "# Get predicted probabilities\n",
        "grid_predict_prob = grid_search.predict_proba(X_test)[:,1]\n",
        "# Get performance metrics\n",
        "precision, recall, fscore, support = score(y_test, grid_predict)\n",
        "# Print result\n",
        "print(f'The recall value for the xgboost grid search is {recall[1]:.4f}')\n",
        "\n",
        "\n",
        "###### Step 4: Random Search for XGBoost\n",
        "# Define the search space\n",
        "param_grid = {\n",
        "    # Learning rate shrinks the weights to make the boosting process more conservative\n",
        "    \"learning_rate\": [0.0001,0.001, 0.01, 0.1, 1] ,\n",
        "    # Maximum depth of the tree, increasing it increases the model complexity.\n",
        "    \"max_depth\": range(3,21,3),\n",
        "    # Gamma specifies the minimum loss reduction required to make a split.\n",
        "    \"gamma\": [i/10.0 for i in range(0,5)],\n",
        "    # Percentage of columns to be randomly samples for each tree.\n",
        "    \"colsample_bytree\": [i/10.0 for i in range(3,10)],\n",
        "    # reg_alpha provides l1 regularization to the weight, higher values result in more conservative models\n",
        "    \"reg_alpha\": [1e-5, 1e-2, 0.1, 1, 10, 100],\n",
        "    # reg_lambda provides l2 regularization to the weight, higher values result in more conservative models\n",
        "    \"reg_lambda\": [1e-5, 1e-2, 0.1, 1, 10, 100]}\n",
        "# Set up score\n",
        "scoring = ['recall']\n",
        "# Set up the k-fold cross-validation\n",
        "kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=0)\n",
        "# Define random search\n",
        "random_search = RandomizedSearchCV(estimator=xgboost,\n",
        "                           param_distributions=param_grid,\n",
        "                           n_iter=48,\n",
        "                           scoring=scoring,\n",
        "                           refit='recall',\n",
        "                           n_jobs=-1,\n",
        "                           cv=kfold,\n",
        "                           verbose=0)\n",
        "# Fit grid search\n",
        "random_result = random_search.fit(X_train, y_train)\n",
        "# Print grid search summary\n",
        "random_result\n",
        "# Print the best score and the corresponding hyperparameters\n",
        "print(f'The best score is {random_result.best_score_:.4f}')\n",
        "print('The best score standard deviation is', round(random_result.cv_results_['std_test_recall'][random_result.best_index_], 4))\n",
        "print(f'The best hyperparameters are {random_result.best_params_}')\n",
        "# Make prediction using the best model\n",
        "random_predict = random_search.predict(X_test)\n",
        "# Get predicted probabilities\n",
        "random_predict_prob = random_search.predict_proba(X_test)[:,1]\n",
        "# Get performance metrics\n",
        "precision, recall, fscore, support = score(y_test, random_predict)\n",
        "# Print result\n",
        "print(f'The recall value for the xgboost random search is {recall[1]:.4f}')\n",
        "\n",
        "\n",
        "\n",
        "###### Step 5: Bayesian Optimization For XGBoost\n",
        "# Space\n",
        "space = {\n",
        "    'learning_rate': hp.choice('learning_rate', [0.0001,0.001, 0.01, 0.1, 1]),\n",
        "    'max_depth' : hp.choice('max_depth', range(3,21,3)),\n",
        "    'gamma' : hp.choice('gamma', [i/10.0 for i in range(0,5)]),\n",
        "    'colsample_bytree' : hp.choice('colsample_bytree', [i/10.0 for i in range(3,10)]),\n",
        "    'reg_alpha' : hp.choice('reg_alpha', [1e-5, 1e-2, 0.1, 1, 10, 100]),\n",
        "    'reg_lambda' : hp.choice('reg_lambda', [1e-5, 1e-2, 0.1, 1, 10, 100])\n",
        "}\n",
        "# Set up the k-fold cross-validation\n",
        "kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=0)\n",
        "# Objective function\n",
        "def objective(params):\n",
        "\n",
        "    xgboost = XGBClassifier(seed=0, **params)\n",
        "    scores = cross_val_score(xgboost, X_train, y_train, cv=kfold, scoring='recall', n_jobs=-1)\n",
        "    # Extract the best score\n",
        "    best_score = max(scores)\n",
        "    # Loss must be minimized\n",
        "    loss = - best_score\n",
        "    # Dictionary with information for evaluation\n",
        "    return {'loss': loss, 'params': params, 'status': STATUS_OK}\n",
        "# Trials to track progress\n",
        "bayes_trials = Trials()\n",
        "# Optimize\n",
        "best = fmin(fn = objective, space = space, algo = tpe.suggest, max_evals = 48, trials = bayes_trials)\n",
        "# Print the index of the best parameters\n",
        "print(best)\n",
        "# Print the values of the best parameters\n",
        "print(space_eval(space, best))\n",
        "# Train model using the best parameters\n",
        "xgboost_bo = XGBClassifier(seed=0,\n",
        "                           colsample_bytree=0.4,\n",
        "                           gamma=0.2,\n",
        "                           learning_rate=1,\n",
        "                           max_depth=12,\n",
        "                           reg_alpha=1e-05,\n",
        "                           reg_lambda=1\n",
        "                           ).fit(X_train,y_train)\n",
        "\n",
        "# Make prediction using the best model\n",
        "bayesian_opt_predict = xgboost_bo.predict(X_test)\n",
        "# Get predicted probabilities\n",
        "bayesian_opt_predict_prob = xgboost_bo.predict_proba(X_test)[:,1]\n",
        "# Get performance metrics\n",
        "precision, recall, fscore, support = score(y_test, bayesian_opt_predict)\n",
        "# Print result\n",
        "print(f'The recall value for the xgboost Bayesian optimization is {recall[1]:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oOhl39NWzjaU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}